{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from etl import safe_literal_eval, movies_df, credits_parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's recover the fields vote_count and voten average from the CSV movie file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_csv = pd.read_csv(\"data/movies_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = movies_df.merge(movies_csv[[\"id\",\"vote_count\", \"vote_average\"]], on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a look at original language field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df[movies_df[\"original_language\"].isna()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And those rows with null values in field original_language also has missing values on other field sudh as vote_count, vote_average, etc.. having 3 features missing is a significant for our EDa.. so that the more suitable approach is drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.dropna(subset=[\"original_language\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### runtime feature missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df[movies_df[\"runtime\"].isna()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some features like revenue, budget, in consecuents roi (shor for return_on_investment), and some vote_count values in zero. Since this is few missing data, the more suitable approach is dropping those rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.dropna(subset=[\"runtime\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collection_id feature\n",
    "collection_id features is an exception here since this refers to more information about certain movies that are related to other ones (there's another dataset from etl that can be imported to merge on this id). A more appropriate feature would be adding a new feture that only indicates if it bellongs to a collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df[\"bellongs_to_collection\"] = ~movies_df[\"collection_id\"].isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now, we will allow the collection_id feature for the EDA, and may be drop it latter when will be time for training the recomendation model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis\n",
    "\n",
    "I would like to know how predominant the language english is in the movies. Parhaps thet would be an outlier. And there are two things about languages: the original language wich the movie was produced and the languages availability for each movie. Let's have a look at this proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df[[\"original_language\", \"spoken_languages\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df[\"spoken_languages\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is odd, can you see in the visualization of the cell above the language \"tl\"? this iso 639 1 code does not have a name.. Let's take a look at those rows by creating a booleant mask for \"tl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_language_mask = movies_df[\"spoken_languages\"].apply(lambda languages: \n",
    "                                                       # This only evauates this: Is \"tl\" in this generator\n",
    "                                                       # The generator can be readen as this: for each disctionary object language in this list of languages,\n",
    "                                                       # throughs the iso_639_1 key in the generator\n",
    "                                                       \"tl\" in (language[\"iso_639_1\"] for language in languages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df[tl_language_mask].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and it seems that \"tl\" shows up as a original_language for any Philipine movie... remind this, in the cell above, we are filtering against spoken_language feature.\n",
    "\n",
    "And after researching at google, it seems \"tl\" iso 3166 1 code is related to the language Tagalog. A language spoken by a quarter of the population of the Philpines. Click (or tap) on [here](https://en.wikipedia.org/wiki/Tagalog_language) for further information. Let's fill this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(   # The movies dataframe filtered only spoken languages that contains the iso_639_1 code \"tl\", \n",
    "    # then grab the spoken language field only\n",
    "    movies_df.loc[tl_language_mask, \"spoken_languages\"]\n",
    "    # Then apply this lambda expression\n",
    "    .apply(lambda \n",
    "           # In the records wich contains a list of dictionaries\n",
    "           languages: \n",
    "           # Evaluate this list comprehention, where for each language in languages, build the dictionary {\"iso_639_1\": \"XXX\", \"name\": \"XXX\"} \n",
    "           # Having the conditionals, in the dictionary constructor, \"name\" key is going to be \"Tagolog\" if the \"iso_639_1\" code evaluates True the value \"tl\"\n",
    "           # else, just write the original value.\n",
    "           [{\"iso_639_1\": language[\"iso_639_1\"], \"name\" : \"Tagolog\" if language[\"iso_639_1\"] == \"tl\" else language[\"name\"]} \n",
    "            for language in languages]\n",
    "            )\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.loc[tl_language_mask,\"spoken_languages\"] =  (movies_df.loc[tl_language_mask, \"spoken_languages\"]\n",
    "                                    .apply(lambda languages: [\n",
    "                                        {\"iso_639_1\": language[\"iso_639_1\"], \"name\" : \"Tagolog\" if language[\"iso_639_1\"] == \"tl\" else language[\"name\"]} \n",
    "                                        for language in languages]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets check the result..\n",
    "movies_df[tl_language_mask].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And let's check the final output.. great, it seems we have already impute Tangolog correctly\n",
    "movies_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but... this only makes me think that probably the \"tl\" iso 639 a code may not be the only one.. let's check if there may be other name in blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df[\"spoken_languages\"].apply(lambda languages: \"\" in (language[\"name\"] for language in languages)).sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep, it is.. and let's see what languages does not have name..\n",
    "\n",
    "The pandas chaining bellow makes this:\n",
    "\n",
    "Filter all rows whose field spoken_languages avualutes an empty string (\"\") in the generator of each language object key name.\n",
    "\n",
    "Then, from the list of languages of each row, give me only the iso 639 1 code whose name is \"\". That list give me the first element, then give me only unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "\n",
    "    movies_df\n",
    "    # Mask with .loc[] paradigm\n",
    "    .loc[movies_df[\"spoken_languages\"]\n",
    "         # In the spocken_languages field:\n",
    "         # The expresion for the validation mask applies this : Is \"\" in this generator for languages name?\n",
    "         # And then grab the \"spoken_language field\" from the movies_df\n",
    "         .apply(lambda languages: \"\" in (language[\"name\"] for language in languages)), \"spoken_languages\"] # ----------> the mask\n",
    "\n",
    "    # After getting the rows with \"\" in any key \"name\" of the dictionary, just create a list with \"name\" key == \"\"\n",
    "    # then only convert the list to a single string by just indexing its only value.\n",
    "    .apply(lambda languages: [language[\"iso_639_1\"] for language in languages if language[\"name\"] == \"\"][0]) # --------> the extraction on iso_639_1\n",
    "# Throws unique values\n",
    ").unique() # --> and this give us the iso_639_1 codes without name.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok.. there were sort of few ones.. running the same code for each language and repeating it over and over will be a drainfull task.. I just asked chatgpt to map those iso 639 1 codes to its language names. And then, we are going to iterate this dictionary to fill the name on those empty keys and that's it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_language_mapping = {\n",
    "    'gd': 'Scottish Gaelic',\n",
    "    'mn': 'Mongolian',\n",
    "    'mk': 'Macedonian',\n",
    "    'kw': 'Cornish',\n",
    "    'nv': 'Navajo',\n",
    "    'mi': 'Maori',\n",
    "    'yi': 'Yiddish',\n",
    "    'ne': 'Nepali',\n",
    "    'km': 'Khmer',\n",
    "    'iu': 'Inuktitut',\n",
    "    'bo': 'Tibetan',\n",
    "    'ty': 'Tahitian',\n",
    "    'si': 'Sinhala',\n",
    "    'as': 'Assamese',\n",
    "    'sh': 'Serbo-Croatian',\n",
    "    'gn': 'Guarani',\n",
    "    'lo': 'Lao',\n",
    "    'xh': 'Xhosa',\n",
    "    'cr': 'Cree',\n",
    "    'ku': 'Kurdish',\n",
    "    'hy': 'Armenian',\n",
    "    'oc': 'Occitan',\n",
    "    'to': 'Tongan',\n",
    "    'ce': 'Chechen',\n",
    "    'qu': 'Quechua',\n",
    "    'am': 'Amharic',\n",
    "    'tg': 'Tajik',\n",
    "    'tt': 'Tatar',\n",
    "    'se': 'Northern Sami',\n",
    "    'ml': 'Malayalam',\n",
    "    'co': 'Corsican',\n",
    "    'dz': 'Dzongkha',\n",
    "    'ht': 'Haitian Creole',\n",
    "    'ln': 'Lingala',\n",
    "    'my': 'Burmese',\n",
    "    'sa': 'Sanskrit',\n",
    "    'fy': 'Western Frisian',\n",
    "    'tk': 'Turkmen',\n",
    "    'ny': 'Chichewa',\n",
    "    'sc': 'Sardinian',\n",
    "    'gu': 'Gujarati',\n",
    "    'mr': 'Marathi',\n",
    "    'ug': 'Uighur',\n",
    "    'ay': 'Aymara',\n",
    "    'st': 'Southern Sotho',\n",
    "    'jv': 'Javanese',\n",
    "    'br': 'Breton',\n",
    "    'sg': 'Sango',\n",
    "    'lb': 'Luxembourgish',\n",
    "    'ab': 'Abkhazian',\n",
    "    'sm': 'Samoan',\n",
    "    'ki': 'Kikuyu',\n",
    "    'tn': 'Tswana',\n",
    "    'fo': 'Faroese',\n",
    "    'sn': 'Shona',\n",
    "    'bi': 'Bislama',\n",
    "    'ig': 'Igbo',\n",
    "    'mh': 'Marshallese'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in iso_language_mapping.items():\n",
    "    language_mask = movies_df[\"spoken_languages\"].apply(lambda languages: key in (language[\"iso_639_1\"] for language in languages))\n",
    "    movies_df.loc[language_mask, \"spoken_languages\"] = (movies_df.loc[language_mask, \"spoken_languages\"]\n",
    "                                        .apply(lambda languages: [\n",
    "                                        {\"iso_639_1\": language[\"iso_639_1\"], \"name\" : value if language[\"iso_639_1\"] == key else language[\"name\"]} \n",
    "                                        for language in languages]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if it worked by searching for those lanuages whose name was this --> \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df[movies_df[\"spoken_languages\"].apply(lambda languages: \"gd\" in (language[\"iso_639_1\"] for language in languages))].loc[108, \"spoken_languages\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "May be, this task had to be in our ETL proccess but recall the main goal of the latter ETL script: end points creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, let's see how numbers are distributed as well as the features entries in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.describe(exclude=\"number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For each numerical feature, count the number of unique entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_numerical_values = movies_df.select_dtypes(include=\"number\").nunique().sort_values()\n",
    "\n",
    "unique_numerical_values.plot.bar(logy=True, figsize=(15, 4), title=\"Unique values per feature\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.imshow(movies_df.isna(), aspect=\"auto\",\n",
    "           interpolation=\"nearest\", cmap=\"gray\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "msno.matrix(movies_df, labels=True, sort=\"descending\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.isna().mean().sort_values().plot(\n",
    "    kind=\"bar\", figsize=(15,4),\n",
    "    title=\"Parcentage of missing values per feature\",\n",
    "    ylabel=\"Ratio of missing value per feature\"\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Distribution\n",
    "\n",
    "This will give us an idea how values are distributed across each feature\n",
    "\n",
    "\n",
    "The right next below figure shows the distrution of the all features that aren't numerical, meaning all categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=1, nrows=3, figsize=(12,8))\n",
    "movies_non_numerical = movies_df.select_dtypes(exclude=[\"number\",\"datetime\"])\n",
    "\n",
    "for col, ax in zip(movies_non_numerical, axes.ravel()):\n",
    "\n",
    "\n",
    "\n",
    "    movies_non_numerical[col].value_counts().plot(\n",
    "\n",
    "        logy=True, title=col, lw=0, marker=\".\", ax=ax\n",
    "    )\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Features Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.hist(bins=25,\n",
    "               figsize=(15, 5),\n",
    "               layout=(-1, 5),\n",
    "               edgecolor=\"black\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_freq_entry = movies_df.mode()\n",
    "\n",
    "df_freq = movies_df.eq(most_freq_entry.values, axis=1).mean().sort_values(ascending=False)\n",
    "\n",
    "display(df_freq.head())\n",
    "\n",
    "df_freq.plot.bar(figsize=(15,4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Ooutliers or Undisirable Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.plot(\n",
    "    lw=0, marker=\".\", subplots=True,\n",
    "    layout=(-1,4), markersize=5, figsize=(15,15)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems we have some points that stand out from the usual range. For example, which movie last above 1200 minutes? lets have a look at it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df[\"runtime\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df[movies_df[\"runtime\"] > 1200] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And certanly Centenial title is not a movie, it is a serie.\n",
    "\n",
    "But in spite of this row -Centennial.. what kind of movie last more than 4 hours?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.loc[movies_df[\"runtime\"] > 240, \"runtime\"].sort_values(ascending=False).hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.loc[movies_df[\"runtime\"].isin(range(240, 400)), \"runtime\"].hist(bins=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.loc[movies_df[\"runtime\"] > 400, \"runtime\"].hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df[movies_df[\"runtime\"] > 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df[movies_df[\"runtime\"].isin(range(400, 1000))][[\"title\", \"release_year\", \"runtime\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's search a random title from this filtering and see if it is a movie.. for example the title [Empire](https://www.filmaffinity.com/es/film975551.html) and this is kind of intereting that, in fact, it is a movie... a movie of 485 minutes... a really low rated one by the way.. this could qualify as outliear because it is not a serie.\n",
    "\n",
    "It is also shown titles such as \"The Godfather Trilogy: 1972-1990\"... is that supposed to be a movie? it seems to be a collection, but in that case, there is a field for collections already.\n",
    "\n",
    "\n",
    "As you can see, spotting outliers is a draining task if you were to search for each record to figure out if it qqualifies as a an outlier... Luckly, there are numerical approach to get done this task.\n",
    "\n",
    "\n",
    "One quick and straightforward option is the boxplot and it is a graph that shows an Interquartile Range (IQR) that in simple words, it divides the data into 4 intervals of the distribution of the standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.boxenplot(data=movies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxenplot(data=movies_df[\"budget\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxenplot(data=movies_df[\"revenue\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxenplot(data=movies_df[\"runtime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxenplot(data=movies_df[\"original_language\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def plot_gauss_belt(data_1d):\n",
    "\n",
    "    mean = np.mean(data_1d)\n",
    "    std_dev = np.std(data_1d)\n",
    "\n",
    "\n",
    "    Q1 = np.percentile(data_1d, 25)\n",
    "    Q3 = np.percentile(data_1d, 75)\n",
    "\n",
    "    x_values = np.linspace(mean - 3 * std_dev, mean + 3 * std_dev, 100)\n",
    "\n",
    "    gaussian = norm.pdf(x_values, mean, std_dev)\n",
    "\n",
    "    plt.plot(x_values, gaussian, label= str(data_1d.name + \" Distribution\"))\n",
    "    plt.axvline(mean, color='b', linestyle='--', label='mean')\n",
    "    plt.axvline(Q1, color='r', linestyle='--', label='Q1')\n",
    "    plt.axvline(Q3, color='g', linestyle='--', label='Q3')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Probability Density')\n",
    "    plt.legend()\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gauss_belt(movies_df[\"runtime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gauss_belt(movies_df[\"budget\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gauss_belt(movies_df[\"revenue\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gauss_belt(movies_df[\"vote_average\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gauss_belt(movies_df[\"vote_count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly there are a few outliers such as budget and revenue where there is some movies really far away of the mean.. may be that unbalance points are explained by  the big capital resourses that hollywood industry has, at least a way more than any other independent movie. \n",
    "\n",
    "May be the data set contains a signicant quantity of independent movies and perhaps the best approach will be clustrization in order to create categories such as \"is this an a independent production movie?\" and maybe that will allow us to get rid of the usage of these features, but this is only an assumption, and we do not have to fall into the temptation of suppose.. let's better have a look at values again.\n",
    "\n",
    "A good question is: how many records in field budget and revenue are zero? having zeros is like having nulls in this case... we need enough data to describe each individual movie. I will copy the dataframe in case of changning something, I'll have no regrest of spoiling something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_draft = movies_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(movies_draft[\"budget\"] == 0).sum() / movies_draft.shape[0] * 100 # around 80% of this feature equals zero.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(movies_draft[\"revenue\"] == 0).sum() / movies_draft.shape[0] * 100 # 83% ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok... definetely, these feature (and in consecuetion return_on_investment) wont be usefull for our machine learning model.. jum... but, this is part of the exploration in the data set, it is nt only about dropping null values and tada: your data is clean and ready for anything you want to do with it.. (may be only clean).. it is more about what makes sense.. and considering these features for our ML model, wont make any sense.. anyway.. let's see what other feature could we get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vote Averge, Vote Count, and Runtime\n",
    "\n",
    "We've already seen how run time values are distributed... and we found that there are some rows that aren't even a movie.. also, we saw that there are movies whsoe runtime is above 7 hours sush as Empire of Andy Warhol or Hitler: A Film from Germany.. they are outliers and they wont be considered for our porpuse. And recalling the gauss standard distribution figure.. the Q3 is almost above 100 minutes runtime.. so we have to drop movies above 200 mintutes runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gauss_belt(movies_df[\"runtime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(movies_df[\"runtime\"] >  200).sum() # only 338 movies to drop.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.shape[0] - movies_df.loc[~(movies_df[\"runtime\"] >  200)].shape[0] # --> this is a simple validation that I am substracting correctly movie above 200 minutues runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = movies_df.loc[~(movies_df[\"runtime\"] >  200)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's see again how results the gauss figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gauss_belt(movies_df[\"runtime\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And.. what about movies whose runtime es bellow 30 mintues.. that's the runtime for a single episode of a ny serie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df[movies_df[\"runtime\"] < 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This filter get rid off all movies runtime equals zero and any production country as \"Unknown\".. yep, it is kind of tricky pandas chaning, but I earn this because I did not want to normalize this feature :)\n",
    "movies_df[(movies_df[\"runtime\"] == 0) \n",
    "          & (movies_df[\"production_countries\"]\n",
    "             # If there were any \"Unknown\" in \"iso_3166_1\" in the generator, it evaluates True\n",
    "             .apply(lambda countries: \"Unknown\" in (country[\"iso_3166_1\"] for country in countries)))] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And the inverse mirror view of the above filtering is this\n",
    "\n",
    "movies_df = movies_df[~((movies_df[\"runtime\"] == 0) \n",
    "            & (movies_df[\"production_countries\"]\n",
    "                # If there were any \"Unknown\" in \"iso_3166_1\" in the generator, it evaluates True\n",
    "                .apply(lambda countries: \"Unknown\" in (country[\"iso_3166_1\"] for country in countries))))] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok.. now I realize that this approach works perfect for trimming data std deviation distribution accross gauss belt by just trimming range of values.\n",
    "plot_gauss_belt(movies_df.loc[movies_df[\"runtime\"].isin(range(45, 120)), \"runtime\"])\n",
    "# Withdrawn values in this trim:\n",
    "print(\"Withdrawn values in this trim: \" + str(movies_df.loc[~movies_df[\"runtime\"].isin(range(45, 120))].shape[0])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok.. it seems this will be usefull for the rest of fetures.. let's conver it into function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trimming_gaus_belt_values(dataframe: pd.DataFrame, feature: str, low_limit: int|float, up_limit: int|float):\n",
    "    \"\"\"\n",
    "    Trim values outside a specified range in a Gaussian distribution belt and visualize the trimmed distribution.\n",
    "    \n",
    "    Parameters:\n",
    "        dataframe (pd.DataFrame): The DataFrame containing the data.\n",
    "        feature (str): The name of the column in the DataFrame representing the feature to trim.\n",
    "        low_limit (int|float): The lower limit of the Gaussian distribution belt.\n",
    "        up_limit (int|float): The upper limit of the Gaussian distribution belt.\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "        \n",
    "    This function trims the values in the specified feature column of the DataFrame that are outside the range\n",
    "    [low_limit, up_limit]. It then plots the trimmed distribution using the plot_gauss_belt function and prints\n",
    "    the number of withdrawn values (values outside the specified range).\n",
    "    \"\"\"\n",
    "    \n",
    "    # this works perfect for trimming data std deviation distribution accross gauss belt by just trimming range of values.\n",
    "    plot_gauss_belt(dataframe.loc[dataframe[feature].isin(range(low_limit, up_limit)), feature])\n",
    "    # Withdrawn values in this trim:\n",
    "    print(\"Withdrawn values in this trimed range\" + str((low_limit, up_limit)) + \": \" \n",
    "          + str(dataframe.loc[~dataframe[feature].isin(range(low_limit, up_limit))].shape[0]) + \"\\n\"\n",
    "          + \"Being the \"\n",
    "          + str(round(dataframe.loc[~dataframe[feature].isin(range(low_limit, up_limit))].shape[0] / dataframe.shape[0] * 100, 2))\n",
    "          + \" % of the data\" + \"\\n\"\n",
    "          + \"The std: \" + str(np.std(dataframe.loc[dataframe[feature].isin(range(low_limit, up_limit)), feature]))\n",
    "          ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df[\"runtime\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for runtime_value in [25, 50, 75, 100]:\n",
    "    trimming_gaus_belt_values(movies_df, feature=\"runtime\", low_limit=runtime_value, up_limit=240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vote average, it is commun vote ranges 0 to 10 or 0 to 5.. but above 10? I we can be sure that is not a reliable value\n",
    "movies_df[\"vote_average\"].describe() # --> ok.. it seems everything ok here.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df[\"vote_count\"].describe() # --> from 0 vores to 14,075 votes.. let's visualize the trimming of the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the std deviation distribution without any trimming\n",
    "plot_gauss_belt(movies_df[\"vote_count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trimming the low limit\n",
    "for vote_count_value in [1, 10, 50, 150]:\n",
    "    trimming_gaus_belt_values(movies_df, feature=\"vote_count\", low_limit=vote_count_value, up_limit=900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trimming the upper limit\n",
    "for vote_count_value in [800, 1600, 2400, 3200]:\n",
    "    trimming_gaus_belt_values(movies_df, feature=\"vote_count\", low_limit=1, up_limit=vote_count_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data points of feature vote_count are really far away from the mean on average... values are highly spread out... this data will make dificult to train a ML model.. at least we are looking for a lightweight model and calculation needs value consistency\n",
    "\n",
    "But I am curious about wich movie has the most of vote_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.loc[movies_df[\"vote_count\"].idxmax()] # Inception, a such good movie by the bay.. this result makes sense.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So.. very intuitively, just by figures, my conclutions:\n",
    "\n",
    "* I will keep this trade off on runtime feature:\n",
    "\n",
    "        Withdrawn values in this trimed range(25, 240): 1932 \n",
    "        Being the 4.38 % of the data\n",
    "        The std: 21.40339410277729\n",
    "\n",
    "* Keep vote_average feature as originall, no changes needed.\n",
    "\n",
    "* Get rid of vote_count feature.. its dispertion is tooo high.. I wont drop it yet.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = movies_df[movies_df[\"runtime\"].isin(range(25, 240))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting more features:\n",
    "\n",
    "Let's explore the characters.. One hypothesis is that depending on the actor weights in decition making.. that's only an hypothesis... I mean, I really like every movie where Gary Oldman is a character in the cast.. Leon: The Professional, The Dark Knight, what a good movies...  \n",
    "\n",
    "And maybe, this step had to be in the ETL script... but I didn't include because I wanted to keep it in the lightest way. The code bellow is commented because this is only executed by my computer in order to have the parquet file needed in the repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Importing data\n",
    "# credits_csv = pd.read_csv(\"data/credits.csv\")\n",
    "# # Checking nulls\n",
    "# credits_csv[\"cast\"].isnull().sum()\n",
    "# # Getting objects\n",
    "# credits_csv[\"cast\"] = credits_csv[\"cast\"].apply(safe_literal_eval)\n",
    "# # Extracting names only\n",
    "# credits_csv[\"cast\"] = credits_csv[\"cast\"].apply(lambda characters: [{\"name\": character[\"name\"]} for character in characters])\n",
    "# # Dropping useless field\n",
    "# credits_csv.drop(columns=[\"crew\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loading into the parquet_data folder repo\n",
    "# credits_csv.to_parquet(\"parquet_data/cast_eda_dataset.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters_df = pd.read_parquet(\"parquet_data/cast_eda_dataset.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.getsizeof(characters_df) / 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters_df[\"id\"] = characters_df[\"id\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters_df[\"cast\"] = characters_df[\"cast\"].apply(lambda actors: [actor[\"name\"] for actor in actors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters_df.merge(movies_df, on=\"id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = characters_df.merge(movies_df, on=\"id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Including feature overview. \n",
    "\n",
    "\n",
    "This feature is kind of large... and it was dropped because including it would not meet the memory constraints of the enviroment for render deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_csv[\"overview\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_csv.dropna(subset=[\"overview\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overviews_df = movies_csv[[\"id\", \"overview\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = movies_df.merge(overviews_df, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Including Genres feature\n",
    "\n",
    "Here we only are going to include de field genre that are in the movies csv file, all of them with its corresponding  movie in a single row... meaning that here we are still going to work with nested data, and handling it by .apply() and lambda expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to keep track of the dataframe shape\n",
    "movies_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_csv[\"genres\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_csv[\"genres\"] = movies_csv[\"genres\"].apply(safe_literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_csv[\"genres\"] = (movies_csv[\"genres\"]\n",
    "                        \n",
    "                        # List only genres names\n",
    "                        .apply(lambda genres: [genre[\"name\"] for genre in genres])\n",
    "                        \n",
    "                        # Convert that listo into a single string by gluing them by \", \"\n",
    "                        .apply(lambda genres: \", \".join(genres)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres_df = movies_csv[[\"id\", \"genres\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = movies_df.merge(genres_df, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formating cast feature\n",
    "\n",
    "I'll also change the feature name to actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df[\"cast\"] = movies_df[\"cast\"].apply(lambda actors: \", \".join(actors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.rename(columns={\"cast\": \"actors\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Content for TF-ADF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df[\"content\"] = movies_df[\"title\"] + \" released in \" + movies_df[\"release_year\"].astype(str) + \" Overview: \" + movies_df[\"overview\"] + \". Actors in the movie: \" + movies_df[\"actors\"] + \". Movie's genres: \" + movies_df[\"genres\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_recomender = movies_df[[\"title\", \"content\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_for_recomender[\"content\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_for_recomender.to_parquet(\"parquet_data/data_for_recommender.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "process = psutil.Process()\n",
    "\n",
    "print(f\"Memory used: {process.memory_info().rss / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build of Recomendation Model\n",
    "\n",
    "This space is to iterate the model and monitor memory consumption. Once get an a suitable model, time to import the parquet data and create the .py file for the FastAPI application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(\"parquet_data/data_for_recommender.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample = data.sample(2000, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "indeces = pd.Series(data_sample.index, index=data_sample[\"title\"]).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = tfidf.fit_transform(data_sample[\"content\"].fillna(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2000x30921 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 121742 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarities_ = linear_kernel(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.000128"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.getsizeof(cosine_similarities_) / 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory used: 224.27 MB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "process = psutil.Process()\n",
    "\n",
    "print(f\"Memory used: {process.memory_info().rss / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(df, column, value, cosine_similarities, limit=10):\n",
    "    \"\"\"Return a dataframe of content recommendations based on TF-IDF cosine similarity.\n",
    "    \n",
    "    Args:\n",
    "        df (object): Pandas dataframe containing the text data. \n",
    "        column (string): Name of column used, i.e. 'title'. \n",
    "        value (string): Name of title to get recommendations for, i.e. 1982 Ferrari 308 GTSi For Sale by Auction\n",
    "        cosine_similarities (array): Cosine similarities matrix from linear_kernel\n",
    "        limit (int, optional): Optional limit on number of recommendations to return. \n",
    "        \n",
    "    Returns: \n",
    "        Pandas dataframe. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Return indices for the target dataframe column and drop any duplicates\n",
    "    indices = pd.Series(df.index, index=df[column]).drop_duplicates()\n",
    "\n",
    "    # Get the index for the target value\n",
    "    target_index = indices[value]\n",
    "\n",
    "    # Get the cosine similarity scores for the target value\n",
    "    cosine_similarity_scores = list(enumerate(cosine_similarities[target_index]))\n",
    "\n",
    "    # Sort the cosine similarities in order of closest similarity\n",
    "    cosine_similarity_scores = sorted(cosine_similarity_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Return tuple of the requested closest scores excluding the target item and index\n",
    "    cosine_similarity_scores = cosine_similarity_scores[1:limit+1]\n",
    "\n",
    "    # Extract the tuple values\n",
    "    index = (x[0] for x in cosine_similarity_scores)\n",
    "    scores = (x[1] for x in cosine_similarity_scores)    \n",
    "\n",
    "    # Get the indices for the closest items\n",
    "    recommendation_indices = [i[0] for i in cosine_similarity_scores]\n",
    "\n",
    "    # Get the actutal recommendations\n",
    "    recommendations = df[column].loc[recommendation_indices]\n",
    "\n",
    "    # Return a dataframe\n",
    "    df = pd.DataFrame(list(zip(index, recommendations, scores)), \n",
    "                      columns=['index','recommendation', 'cosine_similarity_score']) \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_recommendations(df, column, value, cosine_similarities, limit=10):\n",
    "#     \"\"\"\n",
    "#     Return a dataframe of content recommendations based on TF-IDF cosine similarity.\n",
    "\n",
    "#     Args:\n",
    "#         df (object): Pandas dataframe containing the text data.\n",
    "#         column (string): Name of column used, i.e. 'title'.\n",
    "#         value (string): Name of title to get recommendations for, i.e. Toy Story\n",
    "#         cosine_similarities (array): Cosine similarities matrix from linear_kernel\n",
    "#         limit (int, optional): Optional limit on number of recommendations to return.\n",
    "\n",
    "#     Returns:\n",
    "#         Pandas dataframe.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Create a dictionary to map values to indices\n",
    "#     indices = {val: idx for idx, val in enumerate(df[column].unique())}\n",
    "\n",
    "#     # Get the index for the target value\n",
    "#     target_index = indices[value]\n",
    "\n",
    "#     # Get the cosine similarity scores for the target value\n",
    "#     cosine_similarity_scores = list(enumerate(cosine_similarities[target_index]))\n",
    "\n",
    "#     # Sort the cosine similarities in order of closest similarity\n",
    "#     cosine_similarity_scores = sorted(cosine_similarity_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "#     # Return tuple of the requested closest scores excluding the target item and index\n",
    "#     cosine_similarity_scores = cosine_similarity_scores[1:limit+1]\n",
    "\n",
    "#     # Get the indices for the closest items\n",
    "#     recommendation_indices = [i[0] for i in cosine_similarity_scores]\n",
    "\n",
    "#     # Get the actual recommendations\n",
    "#     recommendations = df[column].loc[recommendation_indices].tolist()\n",
    "\n",
    "#     # Get the indices and scores\n",
    "#     index = [x[0] for x in cosine_similarity_scores]\n",
    "#     scores = [x[1] for x in cosine_similarity_scores]\n",
    "\n",
    "#     # Create the dataframe\n",
    "#     df = pd.DataFrame({'index': index, 'recommendation': recommendations, 'cosine_similarity_score': scores})\n",
    "\n",
    "#     return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>recommendation</th>\n",
       "      <th>cosine_similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>326</td>\n",
       "      <td>Toy Story 2</td>\n",
       "      <td>0.467124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>628</td>\n",
       "      <td>Paris-Manhattan</td>\n",
       "      <td>0.126984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1741</td>\n",
       "      <td>Radio Days</td>\n",
       "      <td>0.108983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1841</td>\n",
       "      <td>Mr. Warmth: The Don Rickles Project</td>\n",
       "      <td>0.096097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>693</td>\n",
       "      <td>A Fighter's Blues</td>\n",
       "      <td>0.088095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1282</td>\n",
       "      <td>Skylark</td>\n",
       "      <td>0.080870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>834</td>\n",
       "      <td>Lost and Love</td>\n",
       "      <td>0.076250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>97</td>\n",
       "      <td>Mighty Aphrodite</td>\n",
       "      <td>0.069594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>168</td>\n",
       "      <td>PT 109</td>\n",
       "      <td>0.068901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>759</td>\n",
       "      <td>Surf II</td>\n",
       "      <td>0.068372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                       recommendation  cosine_similarity_score\n",
       "0    326                          Toy Story 2                 0.467124\n",
       "1    628                      Paris-Manhattan                 0.126984\n",
       "2   1741                           Radio Days                 0.108983\n",
       "3   1841  Mr. Warmth: The Don Rickles Project                 0.096097\n",
       "4    693                    A Fighter's Blues                 0.088095\n",
       "5   1282                              Skylark                 0.080870\n",
       "6    834                        Lost and Love                 0.076250\n",
       "7     97                     Mighty Aphrodite                 0.069594\n",
       "8    168                               PT 109                 0.068901\n",
       "9    759                              Surf II                 0.068372"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_recommendations(df=data_sample, column=\"title\", value=\"Toy Story\", cosine_similarities=cosine_similarities_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It did not wrok due to the sample od the data. May be in the sample, Toy Story was not picked.. let's make a litle trick to choice a good random seed that includes Toy Story title. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movies_df.loc[movies_df[\"title\"] == \"Big City Blues\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This just iterates until toy story appears in the data sample. \n",
    "# rand = 0\n",
    "# while (data_sample[data_sample[\"title\"] == \"Toy Story\"].shape[0] > 0) == False:\n",
    "#     data_sample = data_for_recomender.sample(3500, random_state=rand)\n",
    "#     rand += 1\n",
    "\n",
    "# print(rand - 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory used: 224.50 MB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "process = psutil.Process()\n",
    "\n",
    "print(f\"Memory used: {process.memory_info().rss / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proyecto_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
